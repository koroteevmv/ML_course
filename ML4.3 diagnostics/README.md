### Недообучение и переобучение

#### Цель работы

Познакомиться с основными проблемами обобщающей способности алгоритмов машинного обучения: overfitting (переобучение) и underfitting (недообучение).


#### Задания для выполнения

1. Загрузите первые 400 строк прилагающегося датасета `diabetes.csv`.
2. Сделайте количественное описание датасета: число признаков, статистику по признакам.
3. Отделите целевую переменную `Outcome`.
4. Разделите данные на обучающую и валидационную выборки при помощи `train_test_split` из библиотеки `sklearn.model_selection` в соотношении 80-20 (для этого укажите параметр test_size=0.2) с перемешиванием, указав значение параметра `random_state=42`.
5. Создайте объект `DecisionTreeClassifier(random_state=1)`. Обучите модель на обучающих (трейновых) данных. Сделайте предсказание на трейновом и валидационном наборе признаков. Выведите значения метрики `f1-score`для трейнового и валидационного наборов данных. По полученным значениям метрик сделайте предположение о переобученности модели.
6. Произведите кросс-валидацию с использованием функции `cross_validate` из библиотеки  `sklearn.model_selection`. По полученным данным, постройте график зависимости значений `f1-score` от набора данных соответствующей итерации. По графику убедитесь в том, что имеет место переобученность модели.
7. Для борьбы с переобучением регуляризуйте модель `DecisionTreeClassifier`, уменьшив глубину дерева, указав параметр регуляризации `max_depth=3`.
8. Снова проделайте пункт 6 с учётом регуляризации и убелитесь по графику в том, что модель больше не является переобученной.
9. Теперь рассмотрите проблему недообучения модели. Для борьбы с недообучением модели добавьте данные.
Для этого загрузите все строки датасета `diabetes.csv`.
10. Обучите модель `DecisionTreeClassifier(random_state=1, max_depth=3)` на всех данных и убедитесь в том, что значение метрики `f1-score` улучшилось.


#### Методические указания

Как всегда загружаем стандартный набор необходимых библиотек:

```py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

##### Диагностика на синтетических данных

Для первого примера воспользуемся синтетическими данными. Сгенерируем большой и достаточно сложный датасет для классификации. В нем будет 10 тысяч точек и 500 атрибутов. Из них всего 50 признаков будут информативными. Сразу после генерации разделим получившийся датасет на обучающую и тестовую выборки:

```py
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=10000, n_features=500, 
                           n_informative=50, n_repeated=0, 
                           class_sep=1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.25, 
                                                    random_state=3)
```

Обучим на этом наборе данных самую простую модель - логистическую регрессию. Выведем оценки точности этой модели на обучающей и тестовой выборке:

```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression().fit(X_train, y_train)

print(f"Training score: {lr.score(X_train, y_train):.4f}")
print(f"Test score: {lr.score(X_test, y_test):.4f}")
```

В данном случае мы используем метрику эффективности классификации по умолчанию. В зависимости от задания вы можете проводить анализ по любой интересующей вас метрике. Получаем такие оценки эффективности данной модели:

```
Training score: 0.8415
Test score: 0.8056
```

То, насколько отличается обучающая и тестовая эффективность модели - основной индикатор пере- или недообучения. Мы явно видим, что тестовая эффективность полученной модели неидеальна. Определить, что является причиной снижения качества модели - ее излишняя или недостаточная вариативность - основная цель диагностики модели машинного обучения. 

Но по одним этим показателям очень сложно понять, в какой из двух ситуаций мы находимся. Поэтому прибегнем к построению кривых обучения. Это даст нам больше информации о том, каков уровень сложности модели по отношению к данным. Воспользуемся готовой реализацией построения кривых обучения из библиотеки продвинутой визуализации для машинного обучения _yellowbrick_:

```python
from yellowbrick.model_selection import LearningCurve

visualizer = LearningCurve(
    LogisticRegression(), train_sizes=np.linspace(0.5, 1.0, 10)
).fit(X, y).show() 
```

Здесь мы задаем модель, которую хотим проверить и набор долей обучающей выборки, по которым будем ее обучать. В данном случае, эта функция построит модели логистической регрессии, обученные на 10%, 20%, и так далее до 100% обучающей выборки. А затем по каждой модели будет вычислена обучающая и тестовая эффективности. И эти показатели будут изображены на графике, то есть кривой обучения:

![Кривая обучения](https://github.com/koroteevmv/ML_course/blob/2023_new/ML4.3%20diagnostics/ml43-1.png)

Эта кривая показывает, что когда модель учится на небольшом объеме данных, она имеет прекрасную обучающую эффективность, но плохую тестовую. По мере увеличения объема обучающей выборки, эти эффективности сближаются, но между ними все еще сохраняется зазор. 

Напомним, что большой промежуток между тестовой и обучающей эффективностью свидетельствует о переобучении. А низкое значение обучающей эффективности - о недообучении. Так что из этого присутствует в данной модели? Можно предположить, что модель немного переобучается. В пользу этого говорит и то, что наш исходный датасет очень разреженный (хотя мы об этом знаем только потому, что его сгенерировали). Более точно можно будет сказать сравнив эту модель с другими. Самый простой способ - использовать регуляризацию.

МЫ воспользуемся классификатором на основе гребневой регрессии. Вспомним, что обычный метод наименьших квадратов (воплощением которого является модель логистической регрессии) эквивалентна гребневой модели с параметром регуляризации, равном нулю. Чем больше этот параметр, тем больше регуляризации в модели и тем проще получаемые модели. Если мы предполагаем, что наша исходная модель переобучалась, то давайте построим и оценим модель с большим значением параметра регуляризации:

```python
from sklearn.linear_model import RidgeClassifier
lr = RidgeClassifier(alpha=1000000).fit(X_train, y_train)

print(f"Training score: {lr.score(X_train, y_train):.4f}")
print(f"Test score: {lr.score(X_test, y_test):.4f}")
```

При таком огромном значении регуляризации мы почти гарантированно получим недообученную модель. Поэтому полезно будет сравнить результаты модели выше с этими. Вот что получилось у очень регуляризованной модели:

```
Linear Regression-Training set score: 0.7724
Linear Regression-Test set score: 0.7712
```

Обратите внимание, что эффективность стала заметно ниже. Плюс, значения почти совпадают. То есть, почти никакой разницы не осталось. Это подтверждает наш первоначальный вывод о том, что в первой модели разница между обучающей и тестовой эффективностью получилась довольно большой для данной задачи. Построим кривые обучения для нашей второй модели:

```python
visualizer = LearningCurve(
    RidgeClassifier(alpha=1000000), train_sizes=np.linspace(0.1, 1.0, 10)
).fit(X, y) .show()
```

Получилась совсем другая картина:

![Кривая обучения](https://github.com/koroteevmv/ML_course/blob/2023_new/ML4.3%20diagnostics/ml43-2.png)

При анализе таких графиков надо обязательно обращать внимание на масштаб вертикальной оси. Несмотря на то, что кажется, будто значения в правой части графика (а нам интересны в первую очередь именно конечные значения эффективности, остальные - это просто предыстория) сильно больше, чем на первом, на самом деле они ниже. Данный график очень характерен для недообучения - низкие значения эффективности и практически никакой разницы между тестовой и обучающей эффективностью.

Если нерегуляризованная модель переобучается, а регуляризованная - недообучается, то где-то в промежутке есть оптимальная модель, котор

Для того, чтобы проанализировать данную задачу еще более глубоко построим график зависимости эффективности модели от значения параметра регуляризации. Для этого воспользуется логравномерным распределением:

```python
from sklearn.linear_model import RidgeClassifier
trains = []
tests = []
for i in np.logspace(2, 6, 50):
  ridge = RidgeClassifier(alpha=i).fit(X_train, y_train)
  trains.append(ridge.score(X_train, y_train))
  tests.append(ridge.score(X_test, y_test))

plt.plot(trains)
plt.plot(tests)
```

Данный код построит 50 моделей, различающихся только параметром регуляризации. Он будет лежать в диапазоне от 100 ($10^2$) до 100 000 ($10^6$). Диапазон подбирается эмпирически, интересующий нас интервал значений параметров регуляризации будет разный в других задачах. Поэтому рекомендуется сначала построить модели в очень широких пределах (скажем, от -6 до 6), а затем сужать его настолько, чтобы на графике наглядно проявилась примерно такая картина: 

![Кривая обучения](https://github.com/koroteevmv/ML_course/blob/2023_new/ML4.3%20diagnostics/ml43-3.png)

Здесь мы видим, что при определенных значениях параметра регуляризации тестовая эффективность модели (а нас в конечном итоге интересует именно она) лучше, чем во всех остальных. Подбором или алгоритмически можно найти такое оптимальное значение параметра регуляризации. Оно дает нам модель, которая имеет оптимальный уровень сложности для данного конкретного датасета. Давайте выведем получившиеся показатели:

```python
from sklearn.linear_model import RidgeClassifier
lr = RidgeClassifier(alpha=26500).fit(X_train, y_train)

print(f"Training score: {lr.score(X_train, y_train):.4f}")
print(f"Test score: {lr.score(X_test, y_test):.4f}")
```

Эта модель получилась немного лучше, чем самая первая, модель логистической регрессии:

```
Training score: 0.8311
Test score: 0.8228
```

Это произошло потому, что мы за счет регуляризации убрали негативный эффект переобучения. Давайте построим кривые обучения, чтобы посмотреть, как они выглядят для "хорошей" модели:

![Кривая обучения](https://github.com/koroteevmv/ML_course/blob/2023_new/ML4.3%20diagnostics/ml43-4.png)

Опять же, с оглядкой на масштаб вертикальной оси мы видим, что и уровень эффективности выше, чем в первой модели и разница между двумя эффективностями тоже сократилась. И то и другое - это свидетельство большей обобщающей способности модели.

##### Диагностика решающих деревьев

Загружаем данные:

```python
import pandas as pd
df = pd.read_csv('diabetes.csv',nrows=400)
df.head()
```
Отделяем целевую переменную :

```python
target = "Outcome"
y = df[target]
X = df.drop(target, axis=1)
```
Разделим данные на обучающую и валидационную выборки:

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

Создадим объект дерево решений и обучим его на трейновых данных:

```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(random_state=1)
model.fit(X_train, y_train)
```
Сделаем предсказание на тренировочных данных и на валидационных:

```python
y_train_pred = model.predict(X_train)
y_pred = model.predict(X_test)
```
Выведем значение метрики f1_score на тренировочных данных и на валидационных:

```python
print("Train f1_score = %.4f" % f1_score(y_train, y_train_pred))
print("Test f1_score = %.4f" % f1_score(y_test, y_pred))
```
Сделаем кросс-валидацию и выведем среднее значение метрики:
```python
from sklearn.model_selection import cross_validate
cv_metrics = cross_validate(model, X, y, cv=5, scoring='f1_micro', return_train_score=True)
f1_train = cv_metrics['train_score'].mean()
f1_valid = cv_metrics['test_score'].mean()
print('Train f1-score = {:.4f}'.format(f1_train))
print('Valid f1-score = {:.4f}'.format(f1_valid))
```
По данным кросс-валидации построим график и увидим по нему, что действительно имеет место переобучение:

```python
from matplotlib import pyplot as plt
plt.figure(figsize=(15, 5))
plt.plot(cv_metrics['train_score'], label='train', marker='.')
plt.plot(cv_metrics['test_score'], label='valid', marker='.')
plt.ylim([0.5, 1.5]);
plt.xlabel('CV iteration', fontsize=15)
plt.ylabel('f1-score', fontsize=15)
plt.legend(fontsize=15)
```
Для борьбы с переобучением уменьшим глубину дерева, указав параметр регуляризации `max_depth=3`.
Создадим объект дерево решений и произведём заново кросс-валидацию, выведем средние значения метрик:

```python
model = DecisionTreeClassifier(random_state=1, max_depth=3)
cv_metrics = cross_validate(model, X, y, cv=5, scoring='f1_micro', return_train_score=True)
f1_train = cv_metrics['train_score'].mean()
f1_valid = cv_metrics['test_score'].mean()
print('Train f1-score = {:.4f}'.format(f1_train))
print('Valid f1-score = {:.4f}'.format(f1_valid))
```
Далее заново построим график и убедимся, что переобучения теперь нет. Код для построения графика тот же.

Рассмотрим проблему недообучения. Для борьбы с недообучением добавим данные.
Для этого загрузим все строки датасета `diabetes.csv`.

```python
df = pd.read_csv('diabetes.csv')
```
Обучим модель на всех данных и убедимся в том, что значение метрики `f1-score` улучшилось:

```python
target = "Outcome"
y = df[target]
X = df.drop(target, axis=1)

model = DecisionTreeClassifier(random_state=1,max_depth=3)
cv_metrics = cross_validate(model, X, y, cv=5, scoring='f1_micro', return_train_score=True)
f1_valid = cv_metrics['test_score'].mean()
print('Valid f1-score = {:.4f}'.format(f1_valid))
```

#### Задания для самостоятельного выполнения

1. Повторите диагностику модели классификации синтетических данных с использованием других метрик эффективности - precision, recall, F1. Как изменились результаты?
1. Повторите диагностику с использованием других видов модели - метода опорных векторов (исследуйте влияние параметра С на недо-и переобучение), 

#### Контрольные вопросы

1. Что такое обобщающая способность модели?
2. Что такое переобучение модели?
3. Какие есть способы борьбы с переобучением?
4. Что такое регуляризация модели?
5. Что такое недообучение модели?
6. Какие есть способы борьбы с недообучением?

#### Дополнительные задания

1. Добавьте шкалирование признаков в рассмотренную задачу.
2. Изучите обобщающую способность модели для данных задачи регрессии.